{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e733e470",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import reservoirpy as rpy\n",
    "from reservoirpy.datasets import mackey_glass, narma, lorenz\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from helpers import *\n",
    "\n",
    "rpy.verbosity(0)\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69ca3b8",
   "metadata": {},
   "source": [
    "# Impact of Plasticity-Based Reservoir Adaptation on Spectral Radius and  Performance of ESNs\n",
    "\n",
    "We analyze the effects of pretraining ESNs with different plasticity mechanisms. The synaptic plasticity rules that we consider are *Anti-Oja's*, *Normalized Anti-Hebbian*, *Bienenstock-Cooper-Munroe* (BCM), and *Dual-Threshold BCM* (DT-BCM). Furthermore, we apply *intrinsic plasticity* (IP).\n",
    "\n",
    "We evaluate these pretraining methods on three benchmarks, namely a mildly chaotic Mackey-Glass, a Lorenz attractor, and a 10th-order NARMA series. \n",
    "\n",
    "To find out in which cases and why the methods do or do not improve the ESN's performance, we first examine the influence of the reservoir matrix' spectral radius $\\rho$ on the model performance. Next, we inspect how $\\rho$ is modified through the pretraining and conclude whether the change of $\\rho$ might be related to the change in performance.\n",
    "\n",
    "Some helper functions used in this notebook can be found in `helpers.py`.\n",
    "\n",
    "The ESN that we work with is very general and not optimized to the task at hand. Its parameters are given below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e6cfc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "reservoir_params = {\"units\": 100, \"Win\": rpy.mat_gen.normal, \"W\": rpy.mat_gen.normal, \"sr\": 0.5}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703c7514",
   "metadata": {},
   "source": [
    "## Mildly Chaotic Mackey-Glass Series\n",
    "\n",
    "We start with the task of forecasting a mildly chaotic  ($\\tau = 17$) Mackey-Glass series and generate a training and a test series of length $4000$ and $6000$, respectively. We normalize the training data to zero mean and unit variance, which proved to be beneficial in our case, and normalize the test data with the mean and variance of the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1cc15134",
   "metadata": {},
   "outputs": [],
   "source": [
    "MG_17 = mackey_glass(n_timesteps=10001)\n",
    "\n",
    "U_train_MG, Y_train_MG, U_test_MG, Y_test_MG = normalize_data(MG_17[:4000], MG_17[1:4001], MG_17[4000:10000], MG_17[4001:10001])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e878c3a",
   "metadata": {},
   "source": [
    "We measure the performance of the ESN in terms of the $NRMSE$. Because the result depends on the random initialization of input and reservoir weights, we average the $NRMSE$ over 20 independent runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3b8612",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"results/MG\", exist_ok=True)\n",
    "result_esn = avg_nrmse(n_runs=20, reservoir_params=reservoir_params,\n",
    "                       U_train=U_train_MG, Y_train=Y_train_MG, U_test=U_test_MG, Y_test=Y_test_MG, warmup=500,\n",
    "                       output_file=\"results/MG/result_esn.json\", method_name=\"traditional ESN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b7e33b",
   "metadata": {},
   "source": [
    "In the following, we try to improve the results of the traditional ESN by pretraining it with plasticity mechanisms. The parameters of the plasticity mechanisms are set to similar values as in the paper [Unveiling the role of plasticity rules in reservoir computing](https://www.sciencedirect.com/science/article/pii/S092523122100775X).\n",
    "\n",
    "The results of the differently pretrained networks are saved as JSON files in the directory `results/MG`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d00ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain_SP_conf_list = [{\"epochs\": 10, \"rule\": anti_oja, \"params\": {\"eta\": 1e-5}},\n",
    "                         {\"epochs\": 10, \"rule\": normalized_anti_hebbian, \"params\": {\"eta\": 1e-5}},\n",
    "                         {\"epochs\": 10, \"rule\": bcm, \"params\": {\"eta\": 1e-5}},\n",
    "                         {\"epochs\": 10, \"rule\": dtbcm, \"params\": {\"eta\": 1e-5, \"rho\": 0.1}}]\n",
    "pretrain_IP_conf = {\"epochs\": 50, \"params\": {\"eta\": 1e-6, \"mu\": 0, \"sigma\": 1}}\n",
    "pretrain_conf_list = [(pretrain_SP_conf, None) for pretrain_SP_conf in pretrain_SP_conf_list] + \\\n",
    "                     [(None, pretrain_IP_conf)]\n",
    "output_file_list = [\"results/MG/result_ao.json\", \"results/MG/result_nah.json\", \n",
    "                    \"results/MG/result_bcm.json\", \"results/MG/result_dtbcm.json\",\n",
    "                    \"results/MG/result_IP.json\"]\n",
    "method_name_list = [\"ESN pretrained with Anti-Oja's\", \"ESN pretrained with normalized Anti-Hebbian\",\n",
    "                    \"ESN pretrained with BCM\", \"ESN pretrained with DTBCM\",\n",
    "                    \"ESN pretrained with IP\"]\n",
    "evaluate_all(n_runs=20, reservoir_params=reservoir_params,\n",
    "             U_train=U_train_MG, Y_train=Y_train_MG, U_test=U_test_MG, Y_test=Y_test_MG, warmup=500,\n",
    "             pretrain_conf_list=pretrain_conf_list, return_sr_list=True,\n",
    "             output_file_list=output_file_list, method_name_list=method_name_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a43430d",
   "metadata": {},
   "source": [
    "Now we evaluate which of the pretraining methods significantly changed the results of the traditional ESN. For this evaluation, we use the Mann-Whitney $U$ test because it neither assumes normality nor equal variances. If the mean $NRMSE$ of the traditional ESN is smaller than the one of the pretrained ESN, we test whether the pretraining tends to increase the error. Otherwise, we test whether it tends to decrease it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be39af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict = {}\n",
    "for file_name in os.listdir(\"results/MG\"):\n",
    "    if file_name.endswith(\".json\"):\n",
    "        with open(os.path.join(\"results/MG\", file_name), \"r\") as json_file:\n",
    "            results_dict[file_name[:-5]] = json.load(json_file)\n",
    "            \n",
    "result_esn = results_dict[\"result_esn\"]\n",
    "for file_name, result in results_dict.items():\n",
    "    if result_esn[\"avg_nrmse\"] < result[\"avg_nrmse\"]:\n",
    "        alternative = \"less\"\n",
    "    else:\n",
    "        alternative = \"greater\"\n",
    "    if file_name != \"result_esn\":\n",
    "        evaluate_mannwhitneyu(result_esn[\"nrmse_list\"], result[\"nrmse_list\"], alpha=0.05, alternative=alternative, method_names=[\"esn\", file_name[7:]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca11cf3f",
   "metadata": {},
   "source": [
    "It turns out that pretraining with *Anti-Oja's* and *BCM* rule and with *IP* significantly improved the ESN's performance while the other pretraining methods did not significantly change the results. To find out why this is the case, we first analyze how the reservoir matrix' spectral radius influences the network's performance. The results are saved in the directory `analysis/MG`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ace724c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_nrmse_vs_sr(n_runs=20, reservoir_params=reservoir_params,\n",
    "                 U_train=U_train_MG, Y_train=Y_train_MG, U_test=U_test_MG, Y_test=Y_test_MG, warmup=500,\n",
    "                 output_path=\"analysis/MG\")\n",
    "# TODO: get to exec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a734afd1",
   "metadata": {},
   "source": [
    "The optimal spectral radius is $\\rho = 0.75$.\n",
    "\n",
    "Now, we analyze how the different pretraining methods affect the spectral radius. Because of the different number of epochs we create two separate subplots for the synaptic and the intrinsic plasticity rules. The resulting plot is also saved in `analysis/MG`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f042ee2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict_SP = {}\n",
    "results_dict_IP = {}\n",
    "for file_name in os.listdir(\"results/MG\"):\n",
    "    if file_name.endswith(\".json\") and file_name != \"result_esn.json\":\n",
    "        with open(os.path.join(\"results/MG\", file_name), \"r\") as json_file:\n",
    "            method_name = file_name[:-5].split(\"_\")[1]\n",
    "            if method_name == \"IP\":\n",
    "                results_dict_IP[method_name] = json.load(json_file)\n",
    "            else:\n",
    "                results_dict_SP[method_name] = json.load(json_file)\n",
    "\n",
    "plot_sr_vs_epoch(results_dict_SP, results_dict_IP, sr_opt=0.75, output_file=\"analysis/MG/sr_vs_epoch.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fec8211",
   "metadata": {},
   "source": [
    "We observe the following developments of $\\rho$:\n",
    "\n",
    "- *Anti-Oja's*: $\\rho$ increases from $0.5$ to $0.81~(\\pm 0.27)$.\n",
    "- *Normalized Anti-Hebbian*: $\\rho$ jumps from $0.5$ to $1.04$ and then remains mostly unchanged. In the last epoch, its value is $1.03~(\\pm 0.05)$.\n",
    "- *BCM*: $\\rho$ increases slightly from $0.5$ to $0.53~(\\pm 0.07)$.\n",
    "- *DT-BCM*: $\\rho$ remains mostly unchanged. In the last epoch, its value is $0.5~(\\pm 0.002)$.\n",
    "- *IP*: $\\rho$ increases from $0.5$ to $0.58~(\\pm 0.01)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af064c4",
   "metadata": {},
   "source": [
    "## NARMA Series of Order 10\n",
    "\n",
    "Next, we consider the task of forecasting a 10th order NARMA system. We proceed analogously to the previous task of forecasting the MG series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "34ecd34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "NARMA_10 = narma(n_timesteps=10001, order=10, a1=0.3, a2=0.05, b=1.5, c=0.1)\n",
    "\n",
    "U_train_N10 = (NARMA_10[:4000] - np.mean(NARMA_10[:4000])) / np.std(NARMA_10[:4000])\n",
    "Y_train_N10 = (NARMA_10[1:4001] - np.mean(NARMA_10[1:4001])) / np.std(NARMA_10[1:4001])\n",
    "U_test_N10 = (NARMA_10[4000:10000] - np.mean(NARMA_10[:4000])) / np.std(NARMA_10[:4000])\n",
    "Y_test_N10 = (NARMA_10[4001:10001] - np.mean(NARMA_10[1:4001])) / np.std(NARMA_10[1:4001])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a644dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"results/NARMA\", exist_ok=True)\n",
    "result_esn = avg_nrmse(n_runs=20, reservoir_params=reservoir_params,\n",
    "                       U_train=U_train_N10, Y_train=Y_train_N10, U_test=U_test_N10, Y_test=Y_test_N10, warmup=500,\n",
    "                       output_file=\"results/NARMA/result_esn.json\", method_name=\"traditional ESN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287aa8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain_SP_conf_list = [{\"epochs\": 10, \"rule\": anti_oja, \"params\": {\"eta\": 1e-5}},\n",
    "                         {\"epochs\": 10, \"rule\": normalized_anti_hebbian, \"params\": {\"eta\": 1e-5}},\n",
    "                         {\"epochs\": 10, \"rule\": bcm, \"params\": {\"eta\": 1e-5}},\n",
    "                         {\"epochs\": 10, \"rule\": dtbcm, \"params\": {\"eta\": 1e-5, \"rho\": 0.1}}]\n",
    "pretrain_IP_conf = {\"epochs\": 50, \"params\": {\"eta\": 1e-6, \"mu\": 0, \"sigma\": 1}}\n",
    "pretrain_conf_list = [(pretrain_SP_conf, None) for pretrain_SP_conf in pretrain_SP_conf_list] + \\\n",
    "                     [(None, pretrain_IP_conf)]\n",
    "output_file_list = [\"results/NARMA/result_ao.json\", \"results/NARMA/result_nah.json\", \n",
    "                  \"results/NARMA/result_bcm.json\", \"results/NARMA/result_dtbcm.json\",\n",
    "                  \"results/NARMA/result_IP.json\"]\n",
    "method_name_list = [\"ESN pretrained with Anti-Oja's\", \"ESN pretrained with normalized Anti-Hebbian\",\n",
    "                    \"ESN pretrained with BCM\", \"ESN pretrained with DTBCM\",\n",
    "                    \"ESN pretrained with IP\"]\n",
    "evaluate_all(n_runs=20, reservoir_params=reservoir_params, \n",
    "             U_train=U_train_N10, Y_train=Y_train_N10, U_test=U_test_N10, Y_test=Y_test_N10, warmup=500,\n",
    "             pretrain_conf_list=pretrain_conf_list, return_sr_list=True,\n",
    "             output_file_list=output_file_list, method_name_list=method_name_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282169aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict = {}\n",
    "for file_name in os.listdir(\"results/NARMA\"):\n",
    "    if file_name.endswith(\".json\"):\n",
    "        with open(os.path.join(\"results/NARMA\", file_name), \"r\") as json_file:\n",
    "            results_dict[file_name[:-5]] = json.load(json_file)\n",
    "            \n",
    "result_esn = results_dict[\"result_esn\"]\n",
    "for file_name, result in results_dict.items():\n",
    "    if result_esn[\"avg_nrmse\"] < result[\"avg_nrmse\"]:\n",
    "        alternative = \"less\"\n",
    "    else:\n",
    "        alternative = \"greater\"\n",
    "    if file_name != \"result_esn\":\n",
    "        evaluate_mannwhitneyu(result_esn[\"nrmse_list\"], result[\"nrmse_list\"], alpha=0.05, alternative=alternative, method_names=[\"esn\", file_name[7:]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad49667a",
   "metadata": {},
   "source": [
    "It turns out that pretraining with *Anti-Oja's*, *Normalized Anti-Hebbian* and intrinsic plasticity significantly improved the ESN's performance while the other pretraining methods did not significantly change the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "23c0c606",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_nrmse_vs_sr(n_runs=20, reservoir_params=reservoir_params,\n",
    "                 U_train=U_train_N10, Y_train=Y_train_N10, U_test=U_test_N10, Y_test=Y_test_N10, warmup=500,\n",
    "                 output_path=\"analysis/NARMA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b8e81d",
   "metadata": {},
   "source": [
    "The optimal spectral radius is $\\rho = 0.6$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81fa310",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict_SP = {}\n",
    "results_dict_IP = {}\n",
    "for file_name in os.listdir(\"results/NARMA\"):\n",
    "    if file_name.endswith(\".json\") and file_name != \"result_esn.json\":\n",
    "        with open(os.path.join(\"results/NARMA\", file_name), \"r\") as json_file:\n",
    "            method_name = file_name[:-5].split(\"_\")[1]\n",
    "            if method_name == \"IP\":\n",
    "                results_dict_IP[method_name] = json.load(json_file)\n",
    "            else:\n",
    "                results_dict_SP[method_name] = json.load(json_file)\n",
    "\n",
    "plot_sr_vs_epoch(results_dict_SP, results_dict_IP, sr_opt=0.6, output_file=\"analysis/NARMA/sr_vs_epoch.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beec8181",
   "metadata": {},
   "source": [
    "We observe the following influences of the different pretraining methods on $\\rho$:\n",
    "\n",
    "- *Anti-Oja's*: $\\rho$ increases from $0.5$ to $0.52~(\\pm 0.05)$.\n",
    "- *Normalized Anti-Hebbian*: $\\rho$ jumps from $0.5$ to $1.04$ and then remains mostly unchanged. In the last epoch, its value is $1~(\\pm 0.02)$.\n",
    "- *BCM*: $\\rho$ does not really change and always remains between $0.5$ and $0.502$. In the last epoch, its value is $0.502~(\\pm 0.01)$.\n",
    "- *DT-BCM*: $\\rho$ does not really change. In the last epoch, its value is $0.5~(\\pm 0.002)$.\n",
    "- *IP*: $\\rho$ increases from $0.5$ to $0.58~(\\pm 0.01)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63dcc90d",
   "metadata": {},
   "source": [
    "## Lorenz Attractor Series\n",
    "\n",
    "Lastly, we analyze the task of forecasting a Lorenz attractor time series. In contrast to the previous two series, this data is not one-, but three-dimensional. We average the $NRMSE$ over the three dimensions and proceed analogously to above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a58f2481",
   "metadata": {},
   "outputs": [],
   "source": [
    "lorenz_att = lorenz(n_timesteps=10001)\n",
    "\n",
    "U_train_LA = (lorenz_att[:4000] - np.mean(lorenz_att[:4000], axis=0)) / np.std(lorenz_att[:4000], axis=0)\n",
    "Y_train_LA = (lorenz_att[1:4001] - np.mean(lorenz_att[1:4001], axis=0)) / np.std(lorenz_att[1:4001], axis=0)\n",
    "U_test_LA = (lorenz_att[4000:10000] - np.mean(lorenz_att[:4000], axis=0)) / np.std(lorenz_att[0:4000], axis=0)\n",
    "Y_test_LA = (lorenz_att[4001:10001] - np.mean(lorenz_att[1:4001], axis=0)) / np.std(lorenz_att[1:4001], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873d7bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"results/lorenz\", exist_ok=True)\n",
    "result_esn = avg_nrmse(n_runs=20, reservoir_params=reservoir_params,\n",
    "                       U_train=U_train_LA, Y_train=Y_train_LA, U_test=U_test_LA, Y_test=Y_test_LA, warmup=500,\n",
    "                       output_file=\"results/lorenz/result_esn.json\", method_name=\"traditional ESN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c798ff4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain_SP_conf_list = [{\"epochs\": 10, \"rule\": anti_oja, \"params\": {\"eta\": 1e-5}},\n",
    "                         {\"epochs\": 10, \"rule\": normalized_anti_hebbian, \"params\": {\"eta\": 1e-5}},\n",
    "                         {\"epochs\": 10, \"rule\": bcm, \"params\": {\"eta\": 1e-5}},\n",
    "                         {\"epochs\": 10, \"rule\": dtbcm, \"params\": {\"eta\": 1e-5, \"rho\": 0.1}}]\n",
    "pretrain_IP_conf = {\"epochs\": 50, \"params\": {\"eta\": 1e-6, \"mu\": 0, \"sigma\": 1}}\n",
    "pretrain_conf_list = [(pretrain_SP_conf, None) for pretrain_SP_conf in pretrain_SP_conf_list] + \\\n",
    "                     [(None, pretrain_IP_conf)]\n",
    "output_file_list = [\"results/lorenz/result_ao.json\", \"results/lorenz/result_nah.json\", \n",
    "                  \"results/lorenz/result_bcm.json\", \"results/lorenz/result_dtbcm.json\",\n",
    "                  \"results/lorenz/result_IP.json\"]\n",
    "method_name_list = [\"ESN pretrained with Anti-Oja's\", \"ESN pretrained with normalized Anti-Hebbian\",\n",
    "                    \"ESN pretrained with BCM\", \"ESN pretrained with DTBCM\",\n",
    "                    \"ESN pretrained with IP\"]\n",
    "evaluate_all(n_runs=20, reservoir_params=reservoir_params, \n",
    "             U_train=U_train_LA, Y_train=Y_train_LA, U_test=U_test_LA, Y_test=Y_test_LA, warmup=500,\n",
    "             pretrain_conf_list=pretrain_conf_list, return_sr_list=True,\n",
    "             output_file_list=output_file_list, method_name_list=method_name_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7122408b",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict = {}\n",
    "for file_name in os.listdir(\"results/lorenz\"):\n",
    "    if file_name.endswith(\".json\"):\n",
    "        with open(os.path.join(\"results/lorenz\", file_name), \"r\") as json_file:\n",
    "            results_dict[file_name[:-5]] = json.load(json_file)\n",
    "            \n",
    "result_esn = results_dict[\"result_esn\"]\n",
    "for file_name, result in results_dict.items():\n",
    "    if result_esn[\"avg_nrmse\"] < result[\"avg_nrmse\"]:\n",
    "        alternative = \"less\"\n",
    "    else:\n",
    "        alternative = \"greater\"\n",
    "    if file_name != \"result_esn\":\n",
    "        evaluate_mannwhitneyu(result_esn[\"nrmse_list\"], result[\"nrmse_list\"], alpha=0.05, alternative=alternative, method_names=[\"esn\", file_name[7:]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b109ff9e",
   "metadata": {},
   "source": [
    "It turns out that pretraining with *Anti-Oja's* and *Normalized Anti-Hebbian* rule significantly worsened the ESN's performance while pretraining with *BCM* and intrinsic plasticity significantly improved it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b4a521e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_nrmse_vs_sr(n_runs=20, reservoir_params=reservoir_params,\n",
    "                 U_train=U_train_LA, Y_train=Y_train_LA, U_test=U_test_LA, Y_test=Y_test_LA, warmup=500,\n",
    "                 output_path=\"analysis/lorenz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a207b74a",
   "metadata": {},
   "source": [
    "The optimal spectral radius is $\\rho=0.4$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c42b6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict_SP = {}\n",
    "results_dict_IP = {}\n",
    "for file_name in os.listdir(\"results/lorenz\"):\n",
    "    if file_name.endswith(\".json\") and file_name != \"result_esn.json\":\n",
    "        with open(os.path.join(\"results/lorenz\", file_name), \"r\") as json_file:\n",
    "            method_name = file_name[:-5].split(\"_\")[1]\n",
    "            if method_name == \"IP\":\n",
    "                results_dict_IP[method_name] = json.load(json_file)\n",
    "            else:\n",
    "                results_dict_SP[method_name] = json.load(json_file)\n",
    "\n",
    "plot_sr_vs_epoch(results_dict_SP, results_dict_IP, sr_opt=0.4, output_file=\"analysis/lorenz/sr_vs_epoch.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eec4f2b",
   "metadata": {},
   "source": [
    "We observe the following influences of the different pretraining methods on $\\rho$:\n",
    "\n",
    "- *Anti-Oja's*: $\\rho$ increases from $0.5$ to $1.33~(\\pm 0.19)$.\n",
    "- *Normalized Anti-Hebbian*: $\\rho$ jumps from $0.5$ to $1.06$ where it remains mostly unchanged over a few epochs until it continues to increase to $1.13~(\\pm 0.13)$.\n",
    "- *BCM*: $\\rho$ increases from $0.5$ to $0.6~(\\pm 0.11)$.\n",
    "- *DT-BCM*: $\\rho$ does not really change. In the last epoch, its value is $0.5~(\\pm 0.01)$.\n",
    "- *IP*: $\\rho$ increases from $0.5$ to $0.56~(\\pm 0.01)$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "plasticity_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
